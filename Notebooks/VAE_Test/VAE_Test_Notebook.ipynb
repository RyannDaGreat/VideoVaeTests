{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTX-2 VAE Test Notebook\n",
    "\n",
    "**VAE Specifications:**\n",
    "- Spatial compression: 32x | Temporal compression: 8x | Latent channels: 128\n",
    "- Frame count: must be `1 + 8*k` | Resolution: must be divisible by 32\n",
    "\n",
    "**Tests:**\n",
    "1. Basic Redecode - encode/decode reconstruction quality\n",
    "2. Checkerboard Latent Blend Sweep - varying block sizes from large to 1x1 XOXOXO\n",
    "3. Latent Quantization Sweep - quantize to 256, 128, 64, 32, 16, 4, 2 levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import rp\n",
    "import torch\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "# Global config\n",
    "IN_NOTEBOOK = rp.running_in_jupyter_notebook()\n",
    "top_dir = rp.get_git_working_dir('.')\n",
    "ltx_src = rp.path_join(top_dir, 'LTX2', 'src')\n",
    "\n",
    "# Add LTX packages to path\n",
    "for pkg in ['ltx-core', 'ltx-trainer', 'ltx-pipelines']:\n",
    "    sys.path.insert(0, rp.path_join(ltx_src, 'packages', pkg, 'src'))\n",
    "\n",
    "from ltx_trainer.model_loader import load_video_vae_encoder, load_video_vae_decoder\n",
    "\n",
    "print(f\"Running in notebook: {IN_NOTEBOOK}\")\n",
    "print(f\"Top directory: {top_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = rp.path_join(top_dir, 'LTX2', 'models')\n",
    "vae_path = rp.path_join(models_dir, 'ltx-2-19b-distilled.safetensors')\n",
    "device = rp.select_torch_device(prefer_used=True, reserve=True)\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "print(f\"Device: {device}, dtype: {dtype}\")\n",
    "print(\"Loading VAE...\")\n",
    "vae_encoder = load_video_vae_encoder(vae_path, device=device, dtype=dtype)\n",
    "vae_decoder = load_video_vae_decoder(vae_path, device=device, dtype=dtype)\n",
    "print(\"VAE loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10x higher than 'high' (which is 10^7)\n",
    "VIDEO_BITRATE = 100000000\n",
    "\n",
    "\n",
    "def show_video(video, name, framerate=30):\n",
    "    \"\"\"Display in notebook or save to file.\"\"\"\n",
    "    if IN_NOTEBOOK:\n",
    "        rp.display_video(video, framerate=framerate)\n",
    "    else:\n",
    "        path = rp.path_join(top_dir, 'Notebooks', f'{name}.mp4')\n",
    "        print(f\"Saving: {path}\")\n",
    "        rp.save_video_mp4(video, path, framerate=framerate, video_bitrate=VIDEO_BITRATE)\n",
    "        return path\n",
    "\n",
    "\n",
    "def encode_video(rgb_video):\n",
    "    \"\"\"Encode RGB video (T,H,W,3) to latent (LT,LC,LH,LW).\"\"\"\n",
    "    numpy_video = rp.as_rgb_images(rgb_video, copy=False)\n",
    "    torch_video = rp.as_torch_images(numpy_video, device=device, dtype=dtype, copy=False)\n",
    "    torch_video = rearrange(torch_video, 'T C H W -> 1 C T H W') * 2 - 1\n",
    "    with torch.inference_mode():\n",
    "        latent = vae_encoder(torch_video)\n",
    "    return rearrange(latent, '1 LC LT LH LW -> LT LC LH LW')\n",
    "\n",
    "\n",
    "def decode_video(latent_video):\n",
    "    \"\"\"Decode latent (LT,LC,LH,LW) to RGB video (T,H,W,3).\"\"\"\n",
    "    latent = rearrange(latent_video, 'LT LC LH LW -> 1 LC LT LH LW')\n",
    "    with torch.inference_mode():\n",
    "        decoded = vae_decoder(latent)\n",
    "    decoded = rearrange(decoded, '1 C T H W -> T H W C')\n",
    "    decoded = ((decoded + 1) / 2).clamp(0, 1) * 255\n",
    "    return decoded.to(torch.uint8).cpu().numpy()\n",
    "\n",
    "\n",
    "def redecode_video(rgb_video):\n",
    "    \"\"\"Encode then decode.\"\"\"\n",
    "    return decode_video(encode_video(rgb_video))\n",
    "\n",
    "\n",
    "def make_comparison(videos, labels):\n",
    "    \"\"\"Create labeled side-by-side comparison video.\"\"\"\n",
    "    return rp.horizontally_concatenated_videos(\n",
    "        rp.resize_lists_to_min_len(\n",
    "            rp.resize_videos_to_min_size(\n",
    "                rp.labeled_videos(videos, labels, font='R:Futura')\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = rp.download_to_cache('https://www.pexels.com/download/video/5291434/')\n",
    "numpy_video_boat = rp.load_video(video_path, length=150, show_progress=True)\n",
    "\n",
    "# Resize and crop to VAE requirements\n",
    "numpy_video_boat = rp.resize_images_to_fit(numpy_video_boat, height=512, width=768, allow_growth=False)\n",
    "numpy_video_boat = rp.as_numpy_array(numpy_video_boat)\n",
    "H, W = numpy_video_boat.shape[1:3]\n",
    "numpy_video_boat = rp.crop_images(numpy_video_boat, (H//32)*32, (W//32)*32, origin='center')\n",
    "numpy_video_boat = rp.as_numpy_array(numpy_video_boat)\n",
    "T = len(numpy_video_boat)\n",
    "numpy_video_boat = numpy_video_boat[:1 + 8*((T-1)//8)]\n",
    "print(f\"Boat video: {numpy_video_boat.shape}\")\n",
    "\n",
    "# Create green video\n",
    "T, H, W, _ = numpy_video_boat.shape\n",
    "numpy_video_green = np.zeros((T, H, W, 3), dtype=np.uint8)\n",
    "numpy_video_green[:, :, :, 1] = 255\n",
    "print(f\"Green video: {numpy_video_green.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test 1: Basic Redecode\n",
    "Encode to latent space, decode back. Should be nearly identical to input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.tic()\n",
    "video_redecoded = redecode_video(numpy_video_boat)\n",
    "rp.ptoc('Redecode')\n",
    "\n",
    "show_video(make_comparison(\n",
    "    [numpy_video_boat, video_redecoded],\n",
    "    ['Input', 'Redecoded']\n",
    "), 'test1_redecode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test 2: Checkerboard Latent Blend Sweep\n",
    "Blend boat and green videos in **latent space** using checkerboard masks of varying sizes, from large blocks down to 1x1 XOXOXO pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.tic()\n",
    "latent_boat = encode_video(numpy_video_boat)\n",
    "latent_green = encode_video(numpy_video_green)\n",
    "rp.ptoc('Encode both')\n",
    "\n",
    "LT, LC, LH, LW = latent_boat.shape\n",
    "print(f\"Latent shape: LT={LT}, LC={LC}, LH={LH}, LW={LW}\")\n",
    "\n",
    "\n",
    "def make_checkerboard_mask(block_h, block_w, LT, LC, LH, LW, device):\n",
    "    \"\"\"Create checkerboard mask with given block size.\"\"\"\n",
    "    mask = torch.zeros((LH, LW), dtype=torch.bool, device=device)\n",
    "    for h in range(LH):\n",
    "        for w in range(LW):\n",
    "            block_row = h // block_h\n",
    "            block_col = w // block_w\n",
    "            if (block_row + block_col) % 2 == 0:\n",
    "                mask[h, w] = True\n",
    "    return mask.unsqueeze(0).unsqueeze(0).expand(LT, LC, LH, LW)\n",
    "\n",
    "\n",
    "# Sweep from large blocks down to 1x1 (XOXOXO pattern)\n",
    "block_sizes = [\n",
    "    (LH // 2, LW // 2),  # 2x2 quadrants\n",
    "    (LH // 4, LW // 4),  # 4x4 blocks\n",
    "    (LH // 8, LW // 8),  # 8x8 blocks\n",
    "    (2, 2),              # 2x2 latent blocks\n",
    "    (1, 1),              # 1x1 XOXOXO\n",
    "]\n",
    "\n",
    "checker_videos = []\n",
    "checker_labels = []\n",
    "\n",
    "for block_h, block_w in block_sizes:\n",
    "    block_h = max(1, block_h)\n",
    "    block_w = max(1, block_w)\n",
    "    label = f'{block_h}x{block_w}'\n",
    "    print(f\"Checkerboard {label}...\")\n",
    "\n",
    "    mask = make_checkerboard_mask(block_h, block_w, LT, LC, LH, LW, device)\n",
    "    latent_blend = torch.where(mask, latent_boat, latent_green)\n",
    "    video_checker = decode_video(latent_blend)\n",
    "\n",
    "    checker_videos.append(video_checker)\n",
    "    checker_labels.append(label)\n",
    "\n",
    "rp.ptoc('All checkerboards')\n",
    "\n",
    "# Create tiled grid\n",
    "grid_video = rp.tiled_videos(\n",
    "    rp.resize_lists_to_min_len(\n",
    "        rp.resize_videos_to_min_size(\n",
    "            rp.labeled_videos(checker_videos, checker_labels, font='R:Futura')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "show_video(grid_video, 'test2_checkerboard_sweep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test 3: Latent Quantization Sweep\n",
    "Quantize latent values to various discrete levels to test compression tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.tic()\n",
    "latent = encode_video(numpy_video_boat)\n",
    "lat_min, lat_max = latent.min(), latent.max()\n",
    "print(f\"Latent range: [{lat_min:.4f}, {lat_max:.4f}]\")\n",
    "\n",
    "\n",
    "def quantize_latent(latent, n_levels):\n",
    "    \"\"\"Quantize latent to n discrete levels.\"\"\"\n",
    "    latent_norm = (latent - lat_min) / (lat_max - lat_min)\n",
    "    latent_quant = (latent_norm * (n_levels - 1)).round() / (n_levels - 1)\n",
    "    return latent_quant * (lat_max - lat_min) + lat_min\n",
    "\n",
    "\n",
    "# Generate videos for each quantization level\n",
    "quant_levels = [256, 128, 64, 32, 16, 4, 2]\n",
    "videos = [numpy_video_boat, video_redecoded]  # Input and Unquantized\n",
    "labels = ['Input', 'Unquantized']\n",
    "\n",
    "for n in quant_levels:\n",
    "    print(f\"Quantizing to {n} levels...\")\n",
    "    latent_q = quantize_latent(latent, n)\n",
    "    video_q = decode_video(latent_q)\n",
    "    videos.append(video_q)\n",
    "    labels.append(f'N={n}')\n",
    "\n",
    "rp.ptoc('All quantizations')\n",
    "\n",
    "# Create tiled grid comparison\n",
    "grid_video = rp.tiled_videos(\n",
    "    rp.resize_lists_to_min_len(\n",
    "        rp.resize_videos_to_min_size(\n",
    "            rp.labeled_videos(videos, labels, font='R:Futura')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "show_video(grid_video, 'test3_quantization_sweep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# All Tests Complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ALL TESTS COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LTX2",
   "language": "python",
   "name": "ltx2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
