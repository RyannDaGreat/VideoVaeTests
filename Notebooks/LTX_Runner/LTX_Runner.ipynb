{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and paths setup\n",
    "import sys\n",
    "import rp\n",
    "import torch\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import shlex\n",
    "from icecream import ic\n",
    "\n",
    "top_dir = rp.get_git_toplevel()\n",
    "\n",
    "ltx_dir = rp.path_join(top_dir, 'LTX2')\n",
    "nfs_models_dir = rp.path_join(ltx_dir, 'models')\n",
    "\n",
    "sys.path+=[\n",
    "    nfs_models_dir,\n",
    "]\n",
    "\n",
    "from download_models import local_download_dir, download_from_web\n",
    "local_models_dir = local_download_dir\n",
    "\n",
    "models_dir = local_models_dir\n",
    "\n",
    "# LTX Pipeline imports\n",
    "from ltx_core.loader import LTXV_LORA_COMFY_RENAMING_MAP, LoraPathStrengthAndSDOps\n",
    "from ltx_pipelines.ti2vid_two_stages import TI2VidTwoStagesPipeline\n",
    "from ltx_pipelines.ic_lora import ICLoraPipeline\n",
    "from ltx_pipelines.utils.media_io import encode_video\n",
    "from ltx_pipelines.utils.constants import AUDIO_SAMPLE_RATE\n",
    "\n",
    "# Model paths\n",
    "checkpoint_path        = rp.path_join(models_dir, \"ltx-2-19b-dev.safetensors\")\n",
    "distilled_lora_path    = rp.path_join(models_dir, \"ltx-2-19b-distilled-lora-resized_dynamic_fro095_avg_rank_242_bf16.safetensors\")\n",
    "spatial_upsampler_path = rp.path_join(models_dir, \"ltx-2-spatial-upscaler-x2-1.0.safetensors\")\n",
    "gemma_root             = models_dir  # Contains text_encoder/ with model files\n",
    "\n",
    "# IC-LoRA paths (for refinement/control)\n",
    "detailer_lora_path     = rp.path_join(models_dir, \"ltx-2-19b-ic-lora-detailer.safetensors\")\n",
    "\n",
    "# Output directory\n",
    "output_dir = rp.path_join(top_dir, \"outputs\")\n",
    "rp.make_directory(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Post-Paths/Imports Setup\n",
    "IN_NOTEBOOK = rp.running_in_jupyter_notebook()\n",
    "DEVICE = rp.select_torch_device(prefer_used=True, reserve=True) #Select the GPU\n",
    "DTYPE = torch.bfloat16\n",
    "\n",
    "download_from_web() #Ensure sure base LTX models are downloaded\n",
    "\n",
    "print(f\"Running in notebook: {IN_NOTEBOOK}\")\n",
    "print(f\"Top directory: {top_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def show_video(video):\n",
    "    if IN_NOTEBOOK:\n",
    "        rp.display_video(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2V (Text-to-Video) Pipeline Setup and Generation\n",
    "distilled_lora = [\n",
    "    LoraPathStrengthAndSDOps(\n",
    "        distilled_lora_path,\n",
    "        0.6,\n",
    "        LTXV_LORA_COMFY_RENAMING_MAP,\n",
    "    ),\n",
    "]\n",
    "\n",
    "pipeline = TI2VidTwoStagesPipeline(\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    distilled_lora=distilled_lora,\n",
    "    spatial_upsampler_path=spatial_upsampler_path,\n",
    "    gemma_root=gemma_root,\n",
    "    loras=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate video from text (T2V - no image conditioning)\n",
    "# NOTE: Two-stage pipeline generates Stage 1 at HALF resolution, then upscales.\n",
    "#       So 1280x768 -> Stage1 at 640x384 -> Upscale to 1280x768\n",
    "#       Dimensions must be divisible by 64 for two-stage pipeline.\n",
    "\n",
    "drone_prompt = \"Drone shot, helicopter flying fast through a narrow rocky canyon, sun-kissed day, clear turquoise water below, white foam waves, motion blur, sharp focus\"\n",
    "\n",
    "negative_prompt = \"worst quality, inconsistent motion, blurry, jittery, distorted, watermarks, low quality, artifacts, morphing, warping, flicker, text, logo\"\n",
    "\n",
    "# Generation parameters\n",
    "height = 768\n",
    "width = 1280\n",
    "num_frames = 121\n",
    "frame_rate = 25.0\n",
    "\n",
    "# Resolution: 1280x768 is 720p-ish, divisible by 64. Stage1 will be 640x384.\n",
    "# For even higher quality, try 1920x1088 (1080p, divisible by 64)\n",
    "ltx_video, ltx_audio = pipeline(\n",
    "    prompt=drone_prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    seed=42,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    num_frames=num_frames,\n",
    "    frame_rate=frame_rate,\n",
    "    num_inference_steps=40,\n",
    "    cfg_guidance_scale=4.0,  # Was 3.0 - 4.0 is recommended for sharper output\n",
    "    images=[],\n",
    ")\n",
    "\n",
    "# Collect video chunks and save with audio using LTX's encoder\n",
    "with torch.inference_mode():\n",
    "    video_chunks = list(ltx_video)\n",
    "    video_tensor = torch.cat(video_chunks, dim=0) if len(video_chunks) > 1 else video_chunks[0]\n",
    "\n",
    "# Save video with audio\n",
    "video_path = rp.path_join(output_dir, \"generated_video.mp4\")\n",
    "encode_video(\n",
    "    video=video_tensor,\n",
    "    fps=int(frame_rate),\n",
    "    audio=ltx_audio,\n",
    "    audio_sample_rate=AUDIO_SAMPLE_RATE,\n",
    "    output_path=video_path,\n",
    "    video_chunks_number=1,\n",
    ")\n",
    "print(f\"Saved video to: {video_path}\")\n",
    "\n",
    "# Convert to numpy for display\n",
    "video = rp.as_numpy_array(video_tensor)\n",
    "print(f\"Output video shape: {video.shape}\")  # Should be ~(121, 768, 1280, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTIONAL: Refine with IC-LoRA Detailer ===\n",
    "# This takes the generated video and enhances fine details/textures.\n",
    "# Uses the saved video_path from the previous cell as conditioning.\n",
    "\n",
    "# Create the detailer pipeline with the IC-LoRA\n",
    "detailer_lora = [\n",
    "    LoraPathStrengthAndSDOps(\n",
    "        detailer_lora_path,\n",
    "        1.0,  # Full strength for detailer\n",
    "        LTXV_LORA_COMFY_RENAMING_MAP,\n",
    "    ),\n",
    "]\n",
    "\n",
    "detailer_pipeline = ICLoraPipeline(\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    spatial_upsampler_path=spatial_upsampler_path,\n",
    "    gemma_root=gemma_root,\n",
    "    loras=detailer_lora,\n",
    ")\n",
    "\n",
    "# Refine the video - uses original video as conditioning\n",
    "detailed_video_gen, detailed_audio = detailer_pipeline(\n",
    "    prompt=drone_prompt,  # Same prompt as original\n",
    "    seed=42,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    num_frames=num_frames,\n",
    "    frame_rate=frame_rate,\n",
    "    images=[],\n",
    "    video_conditioning=[(video_path, 1.0)],  # Condition on our generated video\n",
    ")\n",
    "\n",
    "# Collect video chunks\n",
    "with torch.inference_mode():\n",
    "    detailed_chunks = list(detailed_video_gen)\n",
    "    detailed_tensor = torch.cat(detailed_chunks, dim=0) if len(detailed_chunks) > 1 else detailed_chunks[0]\n",
    "\n",
    "# Save detailed video with audio\n",
    "detailed_video_path = rp.path_join(output_dir, \"generated_video_detailed.mp4\")\n",
    "encode_video(\n",
    "    video=detailed_tensor,\n",
    "    fps=int(frame_rate),\n",
    "    audio=detailed_audio,\n",
    "    audio_sample_rate=AUDIO_SAMPLE_RATE,\n",
    "    output_path=detailed_video_path,\n",
    "    video_chunks_number=1,\n",
    ")\n",
    "print(f\"Saved detailed video to: {detailed_video_path}\")\n",
    "\n",
    "# Convert to numpy for display\n",
    "detailed_video = rp.as_numpy_array(detailed_tensor)\n",
    "print(f\"Detailed video shape: {detailed_video.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the detailed/refined video\n",
    "show_video(detailed_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python LTX2",
   "language": "python",
   "name": "ltx2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
